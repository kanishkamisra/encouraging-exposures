{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c054af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from minicons import scorer\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_constant_schedule,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from experiment import Learner, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9b69bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 4), (0.7, {})), ((1, 2), (0.2, {}))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {\n",
    "    (1,2): (0.2, {}),\n",
    "    (3,4): (0.7, {}),\n",
    "}\n",
    "\n",
    "sorted(x.items(), key = lambda item: item[1][0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6969675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(1024)\n",
    "\n",
    "validation = utils.read_json(\"../../data/experiments/verbhood.json\")\n",
    "\n",
    "generalization = utils.read_jsonl(\"../../data/experiments/generalization.jsonl\")\n",
    "\n",
    "# model = Learner(\n",
    "#     \"kanishka/smolm-aochildes-vocab_8192-layers_8-attn_8-hidden_256-inter_1024-lr_1e-3-seed_1709\",\n",
    "#     device=\"cuda:0\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     \"do you see lucy and me ?\\n<s> lucy [verb] me a red ball .\",\n",
    "#     generalization,\n",
    "#     validation,\n",
    "#     learning_rate=0.02,\n",
    "#     weight_decay=0.01\n",
    "# )\n",
    "# # trainer.add_token()\n",
    "# # trainer.reset()\n",
    "# trainer.train(num_epochs=20, generalization_batch_size=128), len(trainer.metrics['val_performance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d20789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.01; WD: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LR: 0.01; WD: 0.01\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.01; WD: 0.1\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.02; WD: 0.0\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.02; WD: 0.01\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.02; WD: 0.1\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.009; WD: 0.0\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.009; WD: 0.01\n",
      "\n",
      "\n",
      "\n",
      "LR: 0.009; WD: 0.1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.01, 0.02, 0.009]\n",
    "decays = [0.0, 0.01, 0.1]\n",
    "NUM_EPOCHS=70\n",
    "\n",
    "results = defaultdict(tuple)\n",
    "\n",
    "for lr in lrs:\n",
    "    for wd in decays:\n",
    "        print(f\"LR: {lr}; WD: {wd}\")\n",
    "        set_seed(1024)\n",
    "\n",
    "        model = Learner(\n",
    "            \"kanishka/smolm-aochildes-vocab_8192-layers_8-attn_8-hidden_256-inter_1024-lr_1e-3-seed_1709\",\n",
    "            device=\"cuda:0\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            \"do you see lucy and me ?\\n<s> lucy [verb] a red ball to me.\",\n",
    "            generalization,\n",
    "            validation,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=wd\n",
    "        )\n",
    "        trainer.train(num_epochs=NUM_EPOCHS, generalization_batch_size=128), len(trainer.metrics['val_performance'])\n",
    "        results[(lr, wd)] = (trainer.metrics['val_performance'][trainer.best_epoch-1], trainer.agg_gen_results)\n",
    "        print(f\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bc30d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(tuple,\n",
       "            {(0.01, 0.0): (0.4981249880790708,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.557654669790557,\n",
       "                           ('best', 'pp'): -4.7156833489735925})),\n",
       "             (0.01, 0.01): (0.5258091306686401,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.540888644709732,\n",
       "                           ('best', 'pp'): -4.682361414697435})),\n",
       "             (0.01, 0.1): (0.505259478092194,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.38945529388659,\n",
       "                           ('best', 'pp'): -4.593265732129415})),\n",
       "             (0.02, 0.0): (1.4173242521286014,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.382068863782016,\n",
       "                           ('best', 'pp'): -4.711660928196377})),\n",
       "             (0.02, 0.01): (1.4451923251152046,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.322943450465347,\n",
       "                           ('best', 'pp'): -4.664639170964559})),\n",
       "             (0.02, 0.1): (1.3877422666549677,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.043295786597512,\n",
       "                           ('best', 'pp'): -4.477630909283956})),\n",
       "             (0.009, 0.0): (0.4362669479846959,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.554355359799934,\n",
       "                           ('best', 'pp'): -4.677457910113865})),\n",
       "             (0.009, 0.01): (0.43417477726936315,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.548263120651245,\n",
       "                           ('best', 'pp'): -4.676606379614936})),\n",
       "             (0.009, 0.1): (0.43654082655906645,\n",
       "              defaultdict(float,\n",
       "                          {('best', 'do'): -5.448975096326886,\n",
       "                           ('best', 'pp'): -4.601373087035285}))})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d632a2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainer.embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6007ccd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(float,\n",
       "             {('initial', 'do'): -6.521739979946252,\n",
       "              ('initial', 'pp'): -5.627667588657803,\n",
       "              ('best', 'do'): -4.477034922802087,\n",
       "              ('best', 'pp'): -3.8414887494511074}),\n",
       " 0.00796610832214295)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.agg_gen_results,trainer.metrics['val_performance'][trainer.best_epoch-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d746169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0281, -0.1176,  0.0406,  ..., -0.0404, -0.0612,  0.0135],\n",
      "        [ 0.0124, -0.0557,  0.0024,  ...,  0.0176,  0.0403, -0.0465],\n",
      "        [-0.0286, -0.1178,  0.0405,  ..., -0.0407, -0.0608,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0475, -0.0095,  ..., -0.0836,  0.0460, -0.0518],\n",
      "        [-0.0311, -0.1246,  0.0523,  ..., -0.0421, -0.1130,  0.0493],\n",
      "        [-0.0153, -0.0356,  0.0149,  ..., -0.0156, -0.0292,  0.0113]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [00:01<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 20. Validation: 0.012644103765487635\n",
      "Val performance recheck: 0.012644103765487635\n",
      "Parameter containing:\n",
      "tensor([[-0.0280, -0.1174,  0.0406,  ..., -0.0403, -0.0611,  0.0134],\n",
      "        [ 0.0124, -0.0556,  0.0024,  ...,  0.0175,  0.0402, -0.0464],\n",
      "        [-0.0285, -0.1176,  0.0404,  ..., -0.0406, -0.0607,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0474, -0.0095,  ..., -0.0834,  0.0459, -0.0517],\n",
      "        [-0.0311, -0.1244,  0.0522,  ..., -0.0420, -0.1127,  0.0492],\n",
      "        [ 0.0594, -0.0786,  0.1391,  ..., -0.0770,  0.0290,  0.0645]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "set_seed(1024)\n",
    "trainer.reset()\n",
    "trainer.train(num_epochs=20, generalization_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da90b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d30186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0280, -0.1174,  0.0406,  ..., -0.0403, -0.0611,  0.0134],\n",
       "        [ 0.0124, -0.0556,  0.0024,  ...,  0.0175,  0.0402, -0.0464],\n",
       "        [-0.0285, -0.1176,  0.0404,  ..., -0.0406, -0.0607,  0.0137],\n",
       "        ...,\n",
       "        [ 0.0051, -0.0474, -0.0095,  ..., -0.0834,  0.0459, -0.0517],\n",
       "        [-0.0311, -0.1244,  0.0522,  ..., -0.0420, -0.1127,  0.0492],\n",
       "        [-0.0153, -0.0356,  0.0149,  ..., -0.0156, -0.0292,  0.0113]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.lm.model.get_output_embeddings().weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4cec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0594, -0.0786,  0.1391, -0.0720, -0.0173, -0.0145, -0.0123, -0.0721,\n",
       "         -0.0778,  0.1443, -0.0553,  0.1230,  0.0583, -0.0161,  0.0431, -0.1057,\n",
       "          0.0318,  0.0421,  0.1448,  0.0411, -0.1002,  0.0208,  0.0823, -0.0963,\n",
       "          0.1013, -0.0793, -0.0171,  0.0054,  0.0020,  0.0030, -0.1619, -0.0912,\n",
       "         -0.1012,  0.1167,  0.0401, -0.0597, -0.1020,  0.0823,  0.0445, -0.0788,\n",
       "          0.0251,  0.0284, -0.0053, -0.1356, -0.1391,  0.1693, -0.0720, -0.0985,\n",
       "          0.0259,  0.1205,  0.0014,  0.0597, -0.0721,  0.0562, -0.0828, -0.0367,\n",
       "         -0.0367,  0.1403, -0.1406,  0.0050, -0.0153,  0.1719,  0.0559, -0.1277,\n",
       "         -0.0191,  0.0945, -0.0620,  0.0103,  0.1261, -0.0050, -0.0246, -0.0400,\n",
       "          0.0397, -0.0884, -0.0310, -0.1131, -0.0714, -0.0015, -0.0356,  0.0574,\n",
       "          0.1411,  0.1152,  0.0754, -0.1191, -0.0694, -0.1232,  0.0884,  0.0698,\n",
       "          0.0669,  0.0112, -0.0479, -0.0433, -0.0799, -0.0713,  0.0827, -0.0050,\n",
       "          0.0483,  0.0547, -0.1603, -0.1669,  0.0616,  0.0822, -0.0636,  0.0718,\n",
       "         -0.0429,  0.1096,  0.0346,  0.0798, -0.0964, -0.1732, -0.0907,  0.0682,\n",
       "         -0.1643, -0.0431, -0.0663,  0.0299, -0.0577,  0.0195,  0.0863, -0.1020,\n",
       "          0.0113,  0.1117, -0.0037,  0.1151,  0.0311, -0.0235, -0.0078, -0.0411,\n",
       "          0.0362,  0.0712, -0.0345,  0.0348,  0.0143,  0.1024, -0.1025,  0.0051,\n",
       "         -0.0625,  0.0097, -0.1120, -0.0174,  0.1387, -0.0009, -0.0539,  0.0339,\n",
       "         -0.1145, -0.0587, -0.1521,  0.1191,  0.1074,  0.0463, -0.0011, -0.0347,\n",
       "         -0.0729, -0.1434,  0.1006, -0.0943, -0.0603,  0.0362,  0.0306,  0.0228,\n",
       "         -0.1007, -0.0087,  0.1130,  0.0546, -0.0932, -0.1937,  0.0845, -0.0203,\n",
       "          0.0441,  0.0477,  0.0476, -0.0802, -0.0386, -0.0023,  0.0343, -0.0871,\n",
       "          0.0642,  0.0984, -0.0803, -0.1174, -0.0363, -0.0503,  0.0310,  0.0155,\n",
       "         -0.0617, -0.0455, -0.0037,  0.0439, -0.0868,  0.0141, -0.0577, -0.0212,\n",
       "          0.0455,  0.0085, -0.0107,  0.0306, -0.0431, -0.0545,  0.0372,  0.0310,\n",
       "          0.0752, -0.0900, -0.0059,  0.0934, -0.1361, -0.0563,  0.1682,  0.0389,\n",
       "          0.1031,  0.0072,  0.0178, -0.1803, -0.0460,  0.0211, -0.0452,  0.0157,\n",
       "          0.0716, -0.1079, -0.1150, -0.0061, -0.0991,  0.0349, -0.0549, -0.0503,\n",
       "         -0.1297,  0.0587,  0.1079,  0.0352, -0.0590,  0.0357,  0.1501, -0.1022,\n",
       "          0.0320, -0.0549,  0.0597,  0.0555, -0.1048,  0.1885, -0.1592,  0.0574,\n",
       "         -0.0706, -0.0745, -0.0212, -0.0274,  0.0022, -0.0207, -0.0397,  0.1076,\n",
       "          0.0679, -0.0386,  0.1656, -0.0680, -0.0595, -0.0770,  0.0290,  0.0645]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.embs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6addf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.freeze_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b79838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainer.metrics['val_performance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95ba9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbeb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        device=\"cpu\",\n",
    "        gaussian=True,\n",
    "        added_tokens=[\" [verb]\"],\n",
    "        target_params=[\"model.decoder.embed_tokens.weight\"],\n",
    "    ):\n",
    "        \"\"\"Learner Class\"\"\"\n",
    "        self.lm = scorer.IncrementalLMScorer(model_name, device)\n",
    "        self.device = device\n",
    "        self.gaussian = gaussian\n",
    "        self.added_tokens = added_tokens\n",
    "        self.target_params = target_params\n",
    "        self.model_config = {\n",
    "            \"model_name\": model_name,\n",
    "            \"device\": device,\n",
    "            \"gaussian\": gaussian,\n",
    "            \"added_tokens\": added_tokens,\n",
    "            \"target_params\": target_params,\n",
    "        }\n",
    "        self.length = self.lm.model.get_input_embeddings().weight.shape[0]\n",
    "        self.new_length = self.length + len(self.added_tokens)\n",
    "        self.new_index = self.new_length - len(self.added_tokens)\n",
    "\n",
    "    def _initialize_gaussian(self):\n",
    "        embeddings_weight = self.lm.model.get_input_embeddings().weight\n",
    "        embeddings_weight.requires_grad = False\n",
    "\n",
    "        mu = embeddings_weight[: self.new_index].mean(0).detach()\n",
    "        n = self.length\n",
    "        sigma = (\n",
    "            (embeddings_weight[: self.new_index] - mu).T\n",
    "            @ (embeddings_weight[: self.new_index] - mu)\n",
    "        ) / n\n",
    "        dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            mu, covariance_matrix=1e-5 * sigma\n",
    "        )\n",
    "\n",
    "        embeddings_weight[self.new_index :] = torch.stack(\n",
    "            tuple((dist.sample() for _ in range(len(self.added_tokens)))), dim=0\n",
    "        )\n",
    "        embeddings_weight.requires_grad = True\n",
    "\n",
    "    def _freeze(self):\n",
    "        for param in self.lm.model.named_parameters():\n",
    "            if param[0] not in self.target_params:\n",
    "                param[1].requires_grad = False\n",
    "\n",
    "        assert [\n",
    "            param[0]\n",
    "            for param in self.lm.model.named_parameters()\n",
    "            if param[1].requires_grad\n",
    "        ] == self.target_params\n",
    "\n",
    "    def freeze_full(self):\n",
    "        for param in self.lm.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def add_tokens(self):\n",
    "        self.lm.tokenizer.add_tokens(self.added_tokens)\n",
    "        self.lm.model.resize_token_embeddings(self.new_length)\n",
    "        print(\n",
    "            f\"New token added. New embedding size: {self.lm.model.get_output_embeddings().weight.shape}\"\n",
    "        )\n",
    "\n",
    "        if self.gaussian:\n",
    "            self._initialize_gaussian()\n",
    "\n",
    "        self._freeze()\n",
    "        # self.lm.model = self.lm.model.to(self.device)\n",
    "\n",
    "    def add_token_and_reinitialize(self, target_emb):\n",
    "        self.add_tokens()\n",
    "        self.freeze_full()\n",
    "        self.lm.model.get_input_embeddings().weight[self.new_index] = target_emb\n",
    "        # self.lm.model = self.lm.model.to(self.device)\n",
    "\n",
    "    def reinitialize(self, target_emb):\n",
    "        self.lm.model.get_input_embeddings().weight[self.new_index] = target_emb\n",
    "\n",
    "    def prepare_text(self, text, **kwargs):\n",
    "        encoded, offset = self.lm.prepare_text(text=text, **kwargs)\n",
    "        encoded[\"input_ids\"] = torch.tensor(\n",
    "            [\n",
    "                [t - 1 if t > self.length else t for t in token_ids]\n",
    "                for token_ids in encoded.input_ids\n",
    "            ]\n",
    "        )\n",
    "        return encoded, offset\n",
    "\n",
    "    def token_score(\n",
    "        self,\n",
    "        batch,\n",
    "        surprisal=False,\n",
    "        prob=False,\n",
    "        base_two=False,\n",
    "        rank=False,\n",
    "        decode=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For every input sentence, returns a list of tuples in the following format:\n",
    "            `(token, score)`,\n",
    "\n",
    "        where score represents the log-probability (by default) of the token given context. Can also return ranks along with scores.\n",
    "\n",
    "        :param ``Union[str, List[str]]`` batch: a single sentence or a batch of sentences.\n",
    "        :param ``bool`` surprisal: If `True`, returns per-word surprisals instead of log-probabilities.\n",
    "        :param ``bool`` prob: If `True`, returns per-word probabilities instead of log-probabilities.\n",
    "        :param ``bool`` base_two: If `True`, uses log base 2 instead of natural-log (returns bits of values in case of surprisals)\n",
    "        :param ``bool`` rank: If `True`, also returns the rank of each word in context (based on the log-probability value)\n",
    "\n",
    "        :return: A `List` containing a `Tuple` consisting of the word, its associated score, and optionally, its rank.\n",
    "        :rtype: ``Union[List[Tuple[str, float]], List[Tuple[str, float, int]]]``\n",
    "        \"\"\"\n",
    "\n",
    "        assert not (\n",
    "            surprisal and prob\n",
    "        ), \"cannot both evaluate probability and surprisal at the same time!\"\n",
    "        assert not (\n",
    "            base_two and prob\n",
    "        ), \"cannot both use base (which is for a log), and a probability measure at the same time!\"\n",
    "\n",
    "        tokenized = self.prepare_text(batch, **kwargs)\n",
    "        if rank:\n",
    "            scores, ranks = self.lm.compute_stats(\n",
    "                tokenized, rank=rank, prob=prob, base_two=base_two, return_tensors=True\n",
    "            )\n",
    "        else:\n",
    "            scores = self.lm.compute_stats(\n",
    "                tokenized, prob=prob, base_two=base_two, return_tensors=True\n",
    "            )\n",
    "\n",
    "        if surprisal:\n",
    "            scores = [-1.0 * s for s in scores]\n",
    "\n",
    "        scores = [s.tolist() for s in scores]\n",
    "\n",
    "        # indices = [\n",
    "        #     [i for i in indexed if i != self.tokenizer.pad_token_id]\n",
    "        #     for indexed in tokenized[0][\"input_ids\"].tolist()\n",
    "        # ]\n",
    "\n",
    "        indices = [\n",
    "            [i for i, am in zip(instance, attention_mask) if am != 0]\n",
    "            for instance, attention_mask in zip(\n",
    "                tokenized[0][\"input_ids\"].tolist(),\n",
    "                tokenized[0][\"attention_mask\"].tolist(),\n",
    "            )\n",
    "        ]\n",
    "        indices = [[ii + 1 if ii >= self.length else ii for ii in i] for i in indices]\n",
    "        # print(indices)\n",
    "        if decode:\n",
    "            tokens = [self.lm.decode(idx) for idx in indices]\n",
    "        else:\n",
    "            tokens = [self.lm.tokenizer.convert_ids_to_tokens(idx) for idx in indices]\n",
    "\n",
    "        if rank:\n",
    "            assert len(tokens) == len(scores) == len(ranks)\n",
    "        else:\n",
    "            assert len(tokens) == len(scores)\n",
    "\n",
    "        res = []\n",
    "        if rank:\n",
    "            for t, s, r in zip(tokens, scores, ranks):\n",
    "                if len(t) > len(s):\n",
    "                    diff = len(t) - len(s)\n",
    "                    sc = [0.0] * diff + s\n",
    "                    ra = [0] * diff + r\n",
    "                    res.append(list(zip(t, sc, ra)))\n",
    "                else:\n",
    "                    res.append(list(zip(t, sc, ra)))\n",
    "            # return [list(zip(t, s, r)) for t, s, r in zip(tokens, scores, ranks)]\n",
    "        else:\n",
    "            for t, s in zip(tokens, scores):\n",
    "                if len(t) > len(s):\n",
    "                    diff = len(t) - len(s)\n",
    "                    sc = [0.0] * diff + s\n",
    "                    res.append(list(zip(t, sc)))\n",
    "                else:\n",
    "                    res.append(list(zip(t, sc)))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def sequence_score(\n",
    "        self,\n",
    "        batch,\n",
    "        reduction=lambda x: x.mean(0).item(),\n",
    "        prob=False,\n",
    "        base_two=False,\n",
    "        **kw,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Pooled estimates of sequence log probabilities (or some modification of it).\n",
    "\n",
    "        :param batch: a batch of sequences whose score you want to calculate.\n",
    "        :type batch: ``Union[str, List[str]]``\n",
    "        :param reduction: Reduction function, is selected to be\n",
    "            ``lambda x: x.mean(0).item()`` by default, which stands for the avg. log-probability per token for each sequence in the batch.\n",
    "        :type reduction: Callable\n",
    "        :param kw: model-specific keyword arguments to pass to the `prepare_text` function\n",
    "        :return: List of floats specifying the desired score for the stimuli part of the input, e.g., P(stimuli | preamble).\n",
    "        :rtype: ``List[float]``\n",
    "\n",
    "        TODO: reduction should be a string, if it's a function, specify what kind of function. --> how to ensure it is always that type?\n",
    "        \"\"\"\n",
    "        tokenized = self.prepare_text(batch, **kw)\n",
    "        # print(tokenized)\n",
    "        scores = self.lm.compute_stats(\n",
    "            tokenized, rank=False, base_two=base_two, prob=prob, return_tensors=True\n",
    "        )\n",
    "        reduced = list(map(reduction, scores))\n",
    "        return reduced\n",
    "\n",
    "    def logprob(self, corpus, batch_size=-1, by_instance=False):\n",
    "        \"\"\"gets the avg. log prob per token given a corpus.\"\"\"\n",
    "        if batch_size > 0:\n",
    "            scores = []\n",
    "            dl = DataLoader(corpus, batch_size=batch_size)\n",
    "            for batch in dl:\n",
    "                scores.extend(self.sequence_score(batch))\n",
    "        else:\n",
    "            scores = self.sequence_score(corpus)\n",
    "        if by_instance:\n",
    "            return scores\n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65518c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = utils.read_json(\"../../data/experiments/verbhood.json\")\n",
    "\n",
    "generalization = utils.read_jsonl(\"../../data/experiments/generalization.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a4296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        training_set,\n",
    "        generalization_set,\n",
    "        validation_set,\n",
    "        val_performance_metric=\"diff\",\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=0.0,\n",
    "    ):\n",
    "        \"\"\"Trainer Class.\"\"\"\n",
    "        self.model = model\n",
    "        self.model.add_tokens()\n",
    "        self.val_performance_metric = val_performance_metric\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.metrics = {\"train_loss\": [], \"val_performance\": []}\n",
    "        self.training_set = training_set\n",
    "        self.validation_set = validation_set\n",
    "        self.generalization_set = generalization_set\n",
    "        self.generalization_results = []\n",
    "        self.agg_gen_results = defaultdict(float)\n",
    "        self.best_epoch = 70\n",
    "        self.embs = []\n",
    "\n",
    "    def validate(self, batch_size=-1):\n",
    "        # if self.validation set is a json with two cats, then we get\n",
    "        # logprob for both and take diff if thats the metric, or else\n",
    "        # just do pairwise comparison of 1 > 2.\n",
    "        # if validation set is a list of sentences, then we return avg. logprob.\n",
    "        if isinstance(self.validation_set, list):\n",
    "            return self.model.logprob(self.validation_set)\n",
    "        elif isinstance(self.validation_set, dict):\n",
    "            if len(self.validation_set) == 2:\n",
    "                if self.val_performance_metric == \"diff\":\n",
    "                    return self.model.logprob(\n",
    "                        self.validation_set[\"good\"], batch_size=batch_size\n",
    "                    ) - self.model.logprob(\n",
    "                        self.validation_set[\"bad\"], batch_size=batch_size\n",
    "                    )\n",
    "                else:\n",
    "                    num_correct = 0\n",
    "                    goods = self.model.logprob(\n",
    "                        self.validation_set[\"good\"],\n",
    "                        batch_size=batch_size,\n",
    "                        by_instance=True,\n",
    "                    )\n",
    "                    bads = self.model.logprob(\n",
    "                        self.validation_set[\"bad\"],\n",
    "                        batch_size=batch_size,\n",
    "                        by_instance=True,\n",
    "                    )\n",
    "\n",
    "                    for good, bad in zip(goods, bads):\n",
    "                        if good > bad:\n",
    "                            num_correct += 1\n",
    "\n",
    "                    return num_correct / len(self.validation_set[\"good\"])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Validation set must be a list of sentences or a dictionary with two keys (good and bad).\"\n",
    "                )\n",
    "\n",
    "    def optimizer_setup(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.lm.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.lm.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        self.optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, lr=self.learning_rate, eps=1e-8\n",
    "        )\n",
    "        self.scheduler = get_constant_schedule(self.optimizer)\n",
    "\n",
    "    def generalization_step(self, model_state, batch_size=64):\n",
    "        dl = DataLoader(self.generalization_set, batch_size=batch_size, shuffle=False)\n",
    "        results = []\n",
    "        datives = []\n",
    "        for batch in dl:\n",
    "            dative = batch[\"sentence\"]\n",
    "            dative_type = batch[\"dative\"]\n",
    "            datives.extend(dative_type)\n",
    "\n",
    "            scores = self.model.sequence_score(dative)\n",
    "            results.extend(scores)\n",
    "\n",
    "        for i, (res, dat) in enumerate(zip(results, datives)):\n",
    "            # self.generalization_results.append([i + 1, model_state, dat, res])\n",
    "            self.generalization_results.append({\n",
    "                \"item\": i+1,\n",
    "                \"model_state\": model_state,\n",
    "                \"dative\": dat,\n",
    "                \"logprob\": res\n",
    "            })\n",
    "\n",
    "    def aggregate_generalization_results(self):\n",
    "        # check if generalization results has something in it:\n",
    "        assert len(self.generalization_results) != 0\n",
    "\n",
    "        results = defaultdict(lambda : defaultdict(list))\n",
    "        for entry in self.generalization_results:\n",
    "            results[entry['model_state']][entry['dative']].append(entry['logprob'])\n",
    "\n",
    "        results = dict(results)\n",
    "        for state, dative in results.items():\n",
    "            dative = dict(dative)\n",
    "            for d, scores in dative.items():\n",
    "                avg = np.mean(scores)\n",
    "                self.agg_gen_results[(state, d)] = avg\n",
    "\n",
    "    def train(self, num_epochs, generalization_batch_size):\n",
    "        self.generalization_step(\"initial\", generalization_batch_size)\n",
    "        self.optimizer_setup()\n",
    "        encoded, offset = self.model.prepare_text(self.training_set)\n",
    "        encoded = encoded.to(self.model.device)\n",
    "\n",
    "        labels = encoded.input_ids.clone()\n",
    "        if self.model.lm.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.model.lm.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        if self.model.lm.tokenizer.bos_token_id is not None:\n",
    "            labels[labels == 1] = -100\n",
    "\n",
    "        for i in trange(num_epochs, desc=\"Epoch\"):\n",
    "            epoch = i + 1\n",
    "            output = self.model.lm.model(**encoded, labels=labels)\n",
    "            output.loss.backward()\n",
    "\n",
    "            for m, p in self.model.lm.model.named_parameters():\n",
    "                if m in self.model.target_params:\n",
    "                    # embeddings = p\n",
    "                    p.grad[: self.model.new_index] = 0.0\n",
    "                    break\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.model.lm.model.zero_grad()\n",
    "\n",
    "            # store embeddings\n",
    "            emb = (\n",
    "                self.model.lm.model.resize_token_embeddings()\n",
    "                .weight[self.model.new_index :]\n",
    "                .detach()\n",
    "                .clone()\n",
    "            )\n",
    "            # emb.requires_grad = False\n",
    "            self.embs.append(emb)\n",
    "\n",
    "            self.metrics[\"train_loss\"].append(output.loss.item())\n",
    "            self.metrics[\"val_performance\"].append(self.validate())\n",
    "\n",
    "        # self.generalization_step(\"final\", generalization_batch_size)\n",
    "\n",
    "        # print(self.model.lm.model.resize_token_embeddings().weight)\n",
    "\n",
    "        self.best_epoch = np.argmax(self.metrics[\"val_performance\"]) + 1\n",
    "        print(\n",
    "            f\"Best Epoch: {self.best_epoch}. Validation: {self.metrics['val_performance'][self.best_epoch-1]}\"\n",
    "        )\n",
    "        # re-train the model to the best epoch\n",
    "\n",
    "        # reset model to initial state\n",
    "        # self.model.zero_grad()\n",
    "        # self.model.lm.model.eval()\n",
    "        # self.model.requires_grad = False\n",
    "        # self.model.zero_grad()\n",
    "        # print(self.model.lm.model.resize_token_embeddings().weight[\n",
    "        #     self.model.new_index :\n",
    "        # ])\n",
    "        # print(self.embs[self.best_epoch-1])\n",
    "        for m, p in self.model.lm.model.named_parameters():\n",
    "            if m in self.model.target_params:\n",
    "                p.requires_grad = False\n",
    "                # embeddings = p\n",
    "                # p.grad[: self.model.new_index] = 0.0\n",
    "                # break\n",
    "\n",
    "        # print(self.embs[self.best_epoch-1])\n",
    "        # print(self.model.lm.model.resize_token_embeddings().weight[\n",
    "        #     self.model.new_index :\n",
    "        # ] == self.embs[self.best_epoch - 1])\n",
    "\n",
    "        # print(self.embs[self.best_epoch-1])\n",
    "\n",
    "        self.model.lm.model.get_output_embeddings().weight[self.model.new_index :] = (\n",
    "            self.embs[self.best_epoch - 1]\n",
    "        )\n",
    "        # print(self.model.model_config)\n",
    "        # set_seed(42)\n",
    "        # self.model = Learner(**self.model.model_config)\n",
    "        # self.model.add_tokens()\n",
    "\n",
    "        # self.optimizer_setup()\n",
    "        # self.optimizer.zero_grad()\n",
    "        # for i in trange(self.best_epoch, desc=\"Epoch\"):\n",
    "        #     output = self.model.lm.model(**encoded, labels=labels)\n",
    "        #     output.loss.backward()\n",
    "\n",
    "        #     for m, p in self.model.lm.model.named_parameters():\n",
    "        #         if m in self.model.target_params:\n",
    "        #             # embeddings = p\n",
    "        #             p.grad[: self.model.new_index] = 0.0\n",
    "        #             break\n",
    "\n",
    "        #     self.optimizer.step()\n",
    "        #     self.scheduler.step()\n",
    "        #     self.model.lm.model.zero_grad()\n",
    "\n",
    "        print(f\"Val performance recheck: {self.validate()}\")\n",
    "        print(self.model.lm.model.get_output_embeddings().weight)\n",
    "\n",
    "        self.generalization_step(\"best\", generalization_batch_size)\n",
    "\n",
    "        self.aggregate_generalization_results()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4460de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New token added. New embedding size: torch.Size([8193, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch: 100%|██████████| 100/100 [00:06<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 100. Validation: -0.08604023218154921\n",
      "Val performance recheck: -0.08604023218154921\n",
      "Parameter containing:\n",
      "tensor([[-0.0281, -0.1179,  0.0407,  ..., -0.0405, -0.0613,  0.0135],\n",
      "        [ 0.0125, -0.0558,  0.0024,  ...,  0.0176,  0.0404, -0.0466],\n",
      "        [-0.0286, -0.1180,  0.0405,  ..., -0.0408, -0.0610,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0476, -0.0095,  ..., -0.0837,  0.0461, -0.0519],\n",
      "        [-0.0312, -0.1249,  0.0525,  ..., -0.0421, -0.1132,  0.0494],\n",
      "        [ 0.0353, -0.0599,  0.1119,  ...,  0.2288, -0.1100,  0.0599]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = Learner(\n",
    "    \"kanishka/smolm-aochildes-vocab_8192-layers_8-attn_8-hidden_256-inter_1024-lr_1e-3-seed_1709\",\n",
    "    device=\"cuda:0\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    \"do you see lucy with the ball ?\\n<s> lucy [verb] it to a cute dog .\",\n",
    "    generalization,\n",
    "    validation,\n",
    "    learning_rate=0.005,\n",
    ")\n",
    "trainer.train(num_epochs=100, generalization_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bcf9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {('initial', 'do'): -6.521739979946252,\n",
       "             ('initial', 'pp'): -5.627667588657803,\n",
       "             ('best', 'do'): -5.013841332811298,\n",
       "             ('best', 'pp'): -4.412945683797201})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.model.token_score([\"lucy [verb] it to a cute dog .\"])\n",
    "trainer.agg_gen_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b71c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.generalization_step(\"initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.generalization_results\n",
    "# trainer.aggregate_generalization_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f32c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dativegen/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch: 100%|██████████| 100/100 [00:06<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 91. Validation: -0.42660699367523236\n",
      "Val performance recheck: -0.42660699367523236\n",
      "Parameter containing:\n",
      "tensor([[-0.0281, -0.1179,  0.0407,  ..., -0.0405, -0.0613,  0.0135],\n",
      "        [ 0.0125, -0.0558,  0.0024,  ...,  0.0176,  0.0404, -0.0466],\n",
      "        [-0.0286, -0.1180,  0.0405,  ..., -0.0408, -0.0610,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0476, -0.0095,  ..., -0.0837,  0.0461, -0.0519],\n",
      "        [-0.0312, -0.1249,  0.0525,  ..., -0.0421, -0.1132,  0.0494],\n",
      "        [ 0.0290, -0.0254,  0.0722,  ...,  0.0630, -0.0333,  0.0320]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13c0476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9306175160408019,\n",
       " -0.8505882799625395,\n",
       " -0.7908285689353942,\n",
       " -0.7474884629249576,\n",
       " -0.7134220278263097,\n",
       " -0.6831685650348662,\n",
       " -0.6556312465667729,\n",
       " -0.6301717817783361,\n",
       " -0.6079445362091063,\n",
       " -0.589710837602615,\n",
       " -0.5756191515922549,\n",
       " -0.5640258622169494,\n",
       " -0.554303393363953,\n",
       " -0.5464347922801975,\n",
       " -0.5404288601875304,\n",
       " -0.5351755726337437,\n",
       " -0.5309364080429075,\n",
       " -0.5272162830829625,\n",
       " -0.5239656615257262,\n",
       " -0.5209910655021668,\n",
       " -0.5180504345893855,\n",
       " -0.5148744332790374,\n",
       " -0.5116088163852694,\n",
       " -0.5080862390995025,\n",
       " -0.504292080402374,\n",
       " -0.5007421243190766,\n",
       " -0.4970718777179721,\n",
       " -0.49366640686988816,\n",
       " -0.49084588170051546,\n",
       " -0.48819660544395393,\n",
       " -0.48569181323051414,\n",
       " -0.4834382438659661,\n",
       " -0.4815681314468385,\n",
       " -0.4799581658840184,\n",
       " -0.4787719416618348,\n",
       " -0.4778393423557281,\n",
       " -0.4776478064060212,\n",
       " -0.477638304233551,\n",
       " -0.4776538145542144,\n",
       " -0.47753682017326415,\n",
       " -0.4775212001800533,\n",
       " -0.47766709327697754,\n",
       " -0.47794182300567556,\n",
       " -0.4784782385826105,\n",
       " -0.47933374881744406,\n",
       " -0.4805267989635471,\n",
       " -0.4822367393970488,\n",
       " -0.4839903271198276,\n",
       " -0.4858220803737643,\n",
       " -0.4875864863395689,\n",
       " -0.4892913687229159,\n",
       " -0.49074230551719644,\n",
       " -0.4919408190250394,\n",
       " -0.4927673125267029,\n",
       " -0.49328268527984687,\n",
       " -0.4932400131225583,\n",
       " -0.49267604470253,\n",
       " -0.49152457833290075,\n",
       " -0.4895684087276466,\n",
       " -0.48695592284202593,\n",
       " -0.4840758502483373,\n",
       " -0.4810226833820348,\n",
       " -0.47779168725013754,\n",
       " -0.474332385063172,\n",
       " -0.47104480385780345,\n",
       " -0.46795320391654993,\n",
       " -0.4652690434455877,\n",
       " -0.46273037195205635,\n",
       " -0.46038688659667937,\n",
       " -0.458317580223083,\n",
       " -0.45622691392898584,\n",
       " -0.4544095277786253,\n",
       " -0.45272656917572096,\n",
       " -0.4511308383941657,\n",
       " -0.44945076704025233,\n",
       " -0.44774255871772706,\n",
       " -0.446120684146881,\n",
       " -0.4444757688045504,\n",
       " -0.44270746827125507,\n",
       " -0.4408652138710023,\n",
       " -0.43871122717857425,\n",
       " -0.4365225875377652,\n",
       " -0.4343997716903685,\n",
       " -0.43245767712593075,\n",
       " -0.4306948232650756,\n",
       " -0.4293260455131529,\n",
       " -0.42825216412544265,\n",
       " -0.4276052570343021,\n",
       " -0.4271040904521941,\n",
       " -0.4267082321643825,\n",
       " -0.42660699367523236,\n",
       " -0.4269105267524713,\n",
       " -0.427360155582428,\n",
       " -0.42792269587516785,\n",
       " -0.4284875953197478,\n",
       " -0.4292144560813904,\n",
       " -0.4300328600406651,\n",
       " -0.4307826340198515,\n",
       " -0.43162041664123496,\n",
       " -0.4324548637866972]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.metrics['val_performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ffa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dativegen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
