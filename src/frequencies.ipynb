{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f1dfa824854e4885efea024ba22aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3f1c996bd34f9a9710b441bb026c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/118k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0fd8ea55a945bcadf513221815d7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca4322ea32b4401ac06090833c9ae32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/328k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d89b3be48e472195ed957a2ecf6e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93980e62935840f0bf68456dbb201b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kanishka/smolm-autoreg-bpe-seed_111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lexicon/adaptation.json and generalization.json\n",
    "with open(\"../data/lexicon/adaptation.json\") as f:\n",
    "    adaptation = json.load(f)\n",
    "with open(\"../data/lexicon/generalization.json\") as f:\n",
    "    generalization = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_vocab = OrderedSet()\n",
    "for k, v in adaptation.items():\n",
    "    # adaptation_vocab.add(k)\n",
    "    # adaptation_vocab.update(v)\n",
    "    for vv in v:\n",
    "        vv = re.sub(r'(the\\s|little|some|of|red|beautiful|big)+', '', vv).strip()\n",
    "        adaptation_vocab.add(vv)\n",
    "\n",
    "\n",
    "generalization_vocab = OrderedSet()\n",
    "for k, v in generalization.items():\n",
    "    # generalization_vocab.add(k)\n",
    "    # generalization_vocab.update(v)\n",
    "    for vv in v:\n",
    "        vv = re.sub(r'(the\\s|little|some|of|red|beautiful|big|small|bit|a\\s|few|cute|piece)+', '', vv).strip()\n",
    "        generalization_vocab.add(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet(['him', 'them', 'we', 'us', 'he', 'it', 'they', 'cookie monster', 'daddy', 'lucy', 'grandma', 'david', 'barney', 'nemo', 'pooh', 'puppy', 'baby', 'cat', 'book', 'milk', 'soup', 'cake', 'cheerios', 'toys', 'car'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet(['him', 'them', 'we', 'us', 'he', 'it', 'they', 'cookie monster', 'daddy', 'lucy', 'grandma', 'david', 'barney', 'nemo', 'pooh', 'puppy', 'baby', 'cat', 'book', 'milk', 'soup', 'cake', 'cheerios', 'toys', 'car'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adaptation_vocab\n",
    "generalization_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "corpus = []\n",
    "with open(\"../../smolm/data/corpora/babylm_data/babylm_100M/aochildes.train\", \"r\") as f:\n",
    "    for line in f:\n",
    "        corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all the single word frequencies in the corpus\n",
    "word_freq = defaultdict(lambda: 0)\n",
    "for line in corpus:\n",
    "    # words = line.split()\n",
    "    words = tokenizer.tokenize(line)\n",
    "    for word in words:\n",
    "        word = word.replace(\"Ġ\", \"\")\n",
    "        word_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_freq = defaultdict(int)\n",
    "\n",
    "for word in adaptation_vocab:\n",
    "    if \" \" not in word:\n",
    "        # print(word, word_freq[word.lower()])\n",
    "        adaptation_freq[word] = word_freq[word.lower()]\n",
    "    else:\n",
    "        count = 0\n",
    "        for line in corpus:\n",
    "            if word in line:\n",
    "                count += 1\n",
    "        # print(word, count)\n",
    "        adaptation_freq[word] = count\n",
    "\n",
    "adaptation_freq = dict(sorted(adaptation_freq.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 91720,\n",
       " 'I': 72646,\n",
       " 'me': 17834,\n",
       " 'she': 14997,\n",
       " 'mommy': 8675,\n",
       " 'her': 7592,\n",
       " 'ball': 3777,\n",
       " 'bear': 2665,\n",
       " 'bird': 2099,\n",
       " 'dog': 1898,\n",
       " 'juice': 1782,\n",
       " 'doll': 677,\n",
       " 'chocolate': 629,\n",
       " 'teddy': 401,\n",
       " 'grandpa': 396,\n",
       " 'candy': 357,\n",
       " 'elmo': 328,\n",
       " 'kitty': 319,\n",
       " 'bert': 229}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptation_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 91720,\n",
       " 'we': 33200,\n",
       " 'he': 26888,\n",
       " 'they': 16206,\n",
       " 'them': 9131,\n",
       " 'baby': 6830,\n",
       " 'him': 6742,\n",
       " 'daddy': 6644,\n",
       " 'book': 5305,\n",
       " 'car': 4135,\n",
       " 'cat': 2435,\n",
       " 'us': 2404,\n",
       " 'milk': 1911,\n",
       " 'toys': 1470,\n",
       " 'grandma': 717,\n",
       " 'cake': 717,\n",
       " 'puppy': 708,\n",
       " 'soup': 418,\n",
       " 'pooh': 357,\n",
       " 'david': 279,\n",
       " 'cookie monster': 227,\n",
       " 'cheerios': 106,\n",
       " 'lucy': 103,\n",
       " 'nemo': 69,\n",
       " 'barney': 66}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization_freq = defaultdict(int)\n",
    "\n",
    "for word in generalization_vocab:\n",
    "    if \" \" not in word:\n",
    "        # print(word, word_freq[word.lower()])\n",
    "        generalization_freq[word] = word_freq[word.lower()]\n",
    "    else:\n",
    "        count = 0\n",
    "        for line in corpus:\n",
    "            if word in line:\n",
    "                count += 1\n",
    "        # print(word, count)\n",
    "        generalization_freq[word] = count\n",
    "\n",
    "generalization_freq = dict(sorted(generalization_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "generalization_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq['monster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for line in corpus:\n",
    "    if \"cookie monster\" in line:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
